{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A (slightly modified) implementation of LamdaRank as described in [1].\n",
    "#   [1] https://papers.nips.cc/paper/2971-learning-to-rank-with-nonsmooth-cost-functions.pdf\n",
    "#   [2] https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Mathieu Blondel, November 2013\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def dcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    \"\"\"Discounted cumulative gain (DCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score : array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "    gains : str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    DCG @k : float\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    if gains == \"exponential\":\n",
    "        gains = 2 ** y_true - 1\n",
    "    elif gains == \"linear\":\n",
    "        gains = y_true\n",
    "    else:\n",
    "        raise ValueError(\"Invalid gains option.\")\n",
    "\n",
    "    # highest rank is 1 so +2 instead of +1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    \"\"\"Normalized discounted cumulative gain (NDCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score : array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "    gains : str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    NDCG @k : float\n",
    "    \"\"\"\n",
    "    best = dcg_score(y_true, y_true, k, gains)\n",
    "    actual = dcg_score(y_true, y_score, k, gains)\n",
    "    return actual / best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ndcg_score([1,2,3],[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "input_dim = 46\n",
    "data_file = 'data/MQ2007/Fold1/train.txt'\n",
    "data_dir = 'data/MQ2007/Fold1/q_json'\n",
    "data_meta_csv = \"{}/metafile.csv\".format(data_dir)\n",
    "\n",
    "feats_to_drop = ['doc_id','inc','prob','qid','y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# couple of days\n",
    "# day that you recieve the offer \n",
    "# email: \n",
    "# 2 weeks\n",
    "# location: santa-calara, austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_tensor = torch.tensor([1,0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(label_tensor==0).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RANKNET_DS(Dataset):\n",
    "    \"\"\"Document Ranking Dataset.\"\"\"\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file (string): Path to the txt file with q_id.\n",
    "            root_dir (string): Directory with all the query_details.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.meta_file = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.feats_to_drop = feats_to_drop\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_fname = os.path.join(self.root_dir,str(self.meta_file.iloc[idx]['qid']))\n",
    "        q_data = pd.read_csv(\"{}.csv\".format(q_fname))\n",
    "        if self.transform:\n",
    "            return self.transform(q_data,self.feats_to_drop)\n",
    "            \n",
    "        return q_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(q_sample,cols_to_drop):\n",
    "    \"\"\"\n",
    "        input dataframe\n",
    "        transforms datafram into tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    label_tensor = torch.tensor(q_sample['y'].values)\n",
    "    n_rel = (label_tensor!=0).sum().item()\n",
    "    data_tensor = torch.tensor(q_sample[q_sample.columns.difference(cols_to_drop)].values).float()\n",
    "    return label_tensor,data_tensor,n_rel\n",
    "    \n",
    "class DOC_RANK(Dataset):\n",
    "    \"\"\"Document Ranking Dataset.\"\"\"\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file (string): Path to the txt file with q_id.\n",
    "            root_dir (string): Directory with all the query_details.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.meta_file = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.feats_to_drop = feats_to_drop\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_fname = os.path.join(self.root_dir,str(self.meta_file.iloc[idx]['qid']))\n",
    "        q_data = pd.read_csv(\"{}.csv\".format(q_fname))\n",
    "        if self.transform:\n",
    "            return self.transform(q_data,self.feats_to_drop)\n",
    "            \n",
    "        return q_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranknet_ds = RANKNET_DS(data_meta_csv,data_dir,transform)\n",
    "label,dataset,n_rel= ranknet_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(input_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DOC_RANK(data_meta_csv,data_dir,transform)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45086673773561947\n",
      "0.8108582970019788\n",
      "0.9074250374176304\n",
      "0.8532721558012799\n",
      "0.6498600945867898\n",
      "0.589088768612251\n",
      "0.6486218623221718\n",
      "0.9368077570869662\n",
      "0.7965100773735325\n",
      "0.9477344873475498\n",
      "0.8122418648457493\n",
      "0.43893943504566835\n",
      "1.0\n",
      "0.9134166588977729\n",
      "0.544610794872989\n",
      "0.703935114791163\n",
      "0.7686992054208691\n",
      "0.37962081462451813\n",
      "1.0\n",
      "0.7219595910056631\n",
      "0.7453597752277844\n",
      "0.566547889864676\n",
      "0.6241410282065163\n",
      "0.8551595247651802\n",
      "1.0\n",
      "0.901447006056365\n",
      "0.8532721558012799\n",
      "0.8481064169964802\n",
      "0.7091923051268172\n",
      "0.4237902969274442\n",
      "0.7430580004525728\n",
      "0.7606971932189103\n",
      "0.7642678977605577\n",
      "0.8088720434739953\n",
      "0.7182833344196091\n",
      "0.955830517697074\n",
      "0.8340611153182703\n",
      "0.9432379215722527\n",
      "0.7799082337019199\n",
      "0.4639594030703841\n",
      "0.47481757731527635\n",
      "0.4929605712017434\n",
      "0.45298473672387823\n",
      "0.811178322335156\n",
      "0.8340611153182703\n",
      "0.42590829591570295\n",
      "0.8698739994728926\n",
      "0.5076288161633667\n",
      "1.0\n",
      "0.6481044710831649\n",
      "0.6593007041766438\n",
      "0.44879108889881736\n",
      "0.6350677584671\n",
      "0.5821032065260975\n",
      "0.8532721558012799\n",
      "0.7144097119277254\n",
      "0.4199166744355605\n",
      "0.9432379215722527\n",
      "0.7102193294182025\n",
      "0.5323266901845036\n",
      "0.5705181411261643\n",
      "1.0\n",
      "0.37574719213263447\n",
      "0.9510907186004267\n",
      "0.811178322335156\n",
      "0.6873332711195502\n",
      "0.5289442655157581\n",
      "0.8632555551147044\n",
      "0.47094395482339274\n",
      "0.7606971932189103\n",
      "0.7231461552741725\n",
      "0.9368077570869662\n",
      "0.6761895009534851\n",
      "0.6889051188362224\n",
      "0.6982655748662335\n",
      "0.8091026734983539\n",
      "0.6350677584671\n",
      "0.7554797864180021\n",
      "0.5500426749626679\n",
      "0.5336840646124055\n",
      "0.9053206285482487\n",
      "0.6621441992752751\n",
      "0.9074250374176304\n",
      "0.9510907186004267\n",
      "0.6597656324882369\n",
      "1.0\n",
      "0.6345765606439618\n",
      "0.92663607790064\n",
      "0.6315026274115522\n",
      "0.8943286401726793\n",
      "0.5718755155540661\n",
      "0.4953985955231794\n",
      "0.571384317730928\n",
      "0.8532721558012799\n",
      "0.9095430364058891\n",
      "0.5918335904785557\n",
      "0.9053206285482487\n",
      "0.7622690409355825\n",
      "0.6410457898283654\n",
      "0.5151134371263187\n",
      "0.5861584770675266\n",
      "0.5673789497787689\n",
      "0.5151134371263187\n",
      "0.37574719213263447\n",
      "0.9575861412006988\n",
      "1.0\n",
      "0.92663607790064\n",
      "0.484774783212832\n",
      "0.9368077570869662\n",
      "0.8800456786592188\n",
      "0.8088720434739953\n",
      "0.6979570834298975\n",
      "0.7250335242380729\n",
      "0.8564113471486754\n",
      "0.8905202757957813\n",
      "0.4066972554326934\n",
      "0.740982351615771\n",
      "0.4792913643358069\n",
      "0.700887374974622\n",
      "0.8480411588814945\n",
      "0.5270830899677266\n",
      "0.4066972554326934\n",
      "0.7378144002357959\n",
      "0.6136664310699538\n",
      "0.5255664792793981\n",
      "0.762815192207169\n",
      "0.5986319310290642\n",
      "0.9368077570869662\n",
      "0.9095430364058891\n",
      "0.762815192207169\n",
      "0.8000633122016705\n",
      "0.3333333333333333\n",
      "0.6842855313030094\n",
      "0.6441849811757607\n",
      "0.4237902969274442\n",
      "0.8481064169964802\n",
      "0.7256449336246451\n",
      "0.7900799128882461\n",
      "1.0\n",
      "0.5653444929793768\n",
      "0.4199166744355605\n",
      "0.8842222191013388\n",
      "0.8909724089198024\n",
      "0.515724846512891\n",
      "0.5159958223394444\n",
      "0.777299036890523\n",
      "0.9510907186004267\n",
      "0.9432379215722527\n",
      "0.677790108702737\n",
      "0.6108563510886634\n",
      "0.4977003702983908\n",
      "0.8257045171699666\n",
      "0.6018312062458391\n",
      "0.8800456786592188\n",
      "0.42707280183811397\n",
      "0.6449194123202491\n",
      "0.8274601406735915\n",
      "0.5172314361145776\n",
      "0.5217025544447539\n",
      "0.7606971932189103\n",
      "0.8532721558012799\n",
      "0.8108582970019788\n",
      "0.5923509817175624\n",
      "1.0\n",
      "0.8235865181817077\n",
      "0.6678067093893125\n",
      "0.9368077570869662\n",
      "0.7177795332994794\n",
      "0.5541595307759017\n",
      "0.5598943288158169\n",
      "0.37574719213263447\n",
      "0.8066817565598587\n",
      "0.955830517697074\n",
      "0.5821793025312363\n",
      "0.7957756462290443\n",
      "0.5097468151516256\n",
      "0.8532721558012799\n",
      "0.5804236790276114\n",
      "0.8091026734983539\n",
      "1.0\n",
      "0.5500559001289843\n",
      "0.7648908410439711\n",
      "0.6885850935030454\n",
      "0.47218218708801074\n",
      "0.7708688724052365\n",
      "0.3333333333333333\n",
      "0.3775028156362593\n",
      "0.6171085403675856\n",
      "0.9477344873475498\n",
      "0.6176717990221129\n",
      "0.4852269163368532\n",
      "0.8087848080708854\n",
      "0.5018372287141041\n",
      "1.0\n",
      "0.6315026274115522\n",
      "0.6215192280981277\n",
      "0.7659543835545645\n",
      "0.8545365814817667\n",
      "0.5918335904785557\n",
      "0.7406794336655346\n",
      "1.0\n",
      "1.0\n",
      "0.9074250374176304\n",
      "0.8340611153182703\n",
      "0.5947718986560576\n",
      "0.5816619112922294\n",
      "0.677790108702737\n",
      "0.8109900424622541\n",
      "0.6305711926918027\n",
      "0.3333333333333333\n",
      "0.9510907186004267\n",
      "0.7102467616044825\n",
      "0.8532721558012799\n",
      "0.6023217777339953\n",
      "0.8005154453256916\n",
      "0.6891483521575726\n",
      "0.8634438349876061\n",
      "0.3333333333333333\n",
      "0.9074250374176304\n",
      "0.8056273000821934\n",
      "0.6639592803132975\n",
      "0.7276427210494697\n",
      "0.46345933386044075\n",
      "0.8842222191013388\n",
      "0.717792136596471\n",
      "0.6194274225257378\n",
      "0.6909039756611977\n",
      "0.7965100773735325\n",
      "0.8532721558012799\n",
      "0.9477344873475498\n",
      "0.955830517697074\n",
      "1.0\n",
      "0.46988949834572724\n",
      "1.0\n",
      "0.8611375561264455\n",
      "0.6842855313030094\n",
      "0.9069212362975007\n",
      "1.0\n",
      "0.37574719213263447\n",
      "0.5176573596312338\n",
      "0.8122418648457493\n",
      "0.40738844762703974\n",
      "0.48006117753205346\n",
      "0.9432379215722527\n",
      "0.4534995603836646\n",
      "0.618923621405608\n",
      "0.4953985955231794\n",
      "1.0\n",
      "0.5821032065260975\n",
      "0.8485585501205014\n",
      "0.6350677584671\n",
      "0.9074250374176304\n",
      "0.6215192280981277\n",
      "0.8480411588814945\n",
      "0.8376318198599177\n",
      "0.884542244434516\n",
      "0.3333333333333333\n",
      "0.7179804164693728\n",
      "1.0\n",
      "1.0\n",
      "0.9368077570869662\n",
      "0.8585157560180571\n",
      "0.716715990788886\n",
      "0.8311363972596455\n",
      "0.6010699553505001\n",
      "0.5353880201199217\n",
      "1.0\n",
      "0.3649302102017378\n",
      "0.8532721558012799\n",
      "0.5773497457952017\n",
      "0.7276427210494697\n",
      "0.6345765606439618\n",
      "0.92663607790064\n",
      "0.7289071467299565\n",
      "0.5884773592256788\n",
      "0.6550910915065752\n",
      "0.9575861412006988\n",
      "0.6400802742998366\n",
      "0.9575861412006988\n",
      "0.9537125187088151\n",
      "0.8532721558012799\n",
      "0.7568371608459037\n",
      "1.0\n",
      "0.6129319999254654\n",
      "0.6023217777339953\n",
      "1.0\n",
      "0.8017536775903097\n",
      "0.751301668940371\n",
      "0.8400527367984127\n",
      "0.7957756462290443\n",
      "0.8502244159847391\n",
      "0.5010881176720028\n",
      "0.3900954117610807\n",
      "0.9575861412006988\n",
      "0.49507857019000223\n",
      "0.8842222191013388\n",
      "0.9368077570869662\n",
      "0.5021969360736879\n",
      "0.38559884598578353\n",
      "0.8108582970019788\n",
      "1.0\n",
      "0.7979453132134117\n",
      "0.8108582970019788\n",
      "1.0\n",
      "0.6000154988728347\n",
      "0.3480974911723065\n",
      "0.9432379215722527\n",
      "0.7916472565189693\n",
      "1.0\n",
      "0.9368077570869662\n",
      "0.9074250374176304\n",
      "0.8043628744017065\n",
      "0.44236092441353086\n",
      "1.0\n",
      "0.5713717144339363\n",
      "0.5315796557430237\n",
      "0.6708806426217223\n",
      "0.716715990788886\n",
      "0.6056906122838638\n",
      "0.5692537154456775\n",
      "0.6975049503058763\n",
      "0.7877872241459626\n",
      "0.716715990788886\n",
      "0.5861584770675266\n",
      "1.0\n",
      "0.5705181411261643\n",
      "0.37962081462451813\n",
      "0.8532721558012799\n",
      "0.8506629589898831\n",
      "0.8909724089198024\n",
      "0.9074250374176304\n",
      "0.4935973968336144\n",
      "0.8091026734983539\n",
      "0.49806457726291964\n",
      "0.8650111786183293\n",
      "1.0\n",
      "1.0\n",
      "0.640742871878129\n",
      "0.9134166588977729\n",
      "1.0\n",
      "0.8506629589898831\n",
      "0.8066817565598587\n",
      "0.695335283321509\n",
      "0.6087383521004046\n",
      "0.7156842642125638\n",
      "0.4264120970358327\n",
      "0.9368077570869662\n",
      "0.8564113471486754\n",
      "0.6449194123202491\n",
      "0.8532721558012799\n",
      "0.9368077570869662\n",
      "0.44236092441353086\n",
      "0.4957015134734157\n",
      "0.7577724751602855\n",
      "0.8161018972187559\n",
      "1.0\n",
      "0.38559884598578353\n",
      "0.9537125187088151\n",
      "0.3333333333333333\n",
      "0.7953235131050231\n",
      "0.955830517697074\n",
      "0.7791738025574315\n",
      "0.4073737024555802\n",
      "0.6708806426217223\n",
      "0.38224261473290666\n",
      "0.8532721558012799\n",
      "0.37962081462451813\n",
      "0.8013151345851657\n",
      "0.4199166744355605\n",
      "0.6202809958335097\n",
      "0.797441512093282\n",
      "0.5978252119586981\n",
      "0.955830517697074\n",
      "0.8743705652481898\n",
      "0.7606971932189103\n",
      "0.7606971932189103\n",
      "1.0\n",
      "0.7357387513989938\n",
      "0.6426176375450375\n",
      "1.0\n",
      "0.6799978653303831\n",
      "0.8532721558012799\n",
      "0.7953235131050231\n",
      "0.5712525722706526\n",
      "0.4066972554326934\n",
      "0.6023343810309869\n",
      "0.648595668906303\n",
      "0.6194274225257378\n",
      "0.8108582970019788\n",
      "0.6741138521166832\n",
      "1.0\n",
      "0.6305711926918027\n",
      "0.8611511462453226\n",
      "0.955830517697074\n",
      "0.6350677584671\n",
      "0.8800456786592188\n",
      "0.7606971932189103\n",
      "0.5778535469153314\n",
      "0.8585157560180571\n",
      "1.0\n",
      "0.7965100773735325\n",
      "0.9432379215722527\n",
      "0.9432379215722527\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.806984674510095\n",
      "0.44236092441353086\n",
      "0.7333178344604987\n",
      "0.6489247802724082\n",
      "0.5989519563622413\n",
      "0.3333333333333333\n",
      "1.0\n",
      "0.5778535469153314\n",
      "0.654356660362087\n",
      "1.0\n",
      "0.777299036890523\n",
      "0.4865304067164569\n",
      "0.4805523753551915\n",
      "0.38559884598578353\n",
      "0.6789766729712464\n",
      "0.3480974911723065\n",
      "0.9575861412006988\n",
      "0.8634438349876061\n",
      "0.8420631275202292\n",
      "0.3480974911723065\n",
      "0.894393898287665\n",
      "0.4066972554326934\n",
      "1.0\n",
      "0.6943559465628248\n",
      "0.867129177606588\n",
      "0.8532721558012799\n",
      "0.9432379215722527\n",
      "0.8532721558012799\n",
      "0.3775028156362593\n",
      "0.6444156112001194\n",
      "0.9477344873475498\n",
      "0.901447006056365\n",
      "0.9537125187088151\n",
      "1.0\n",
      "0.7708688724052365\n",
      "0.811178322335156\n",
      "0.8532721558012799\n",
      "0.6873332711195502\n",
      "0.4721957772068878\n",
      "0.592136508428792\n",
      "0.9074250374176304\n",
      "0.6741138521166832\n",
      "1.0\n",
      "0.5392480524929283\n",
      "0.7799082337019199\n",
      "0.5541595307759017\n",
      "0.5341361977364265\n",
      "1.0\n",
      "1.0\n",
      "0.49056079604754593\n",
      "1.0\n",
      "0.4957015134734157\n",
      "0.8645073774981995\n",
      "0.7799082337019199\n",
      "0.6889177221332141\n",
      "0.46345933386044075\n",
      "0.955830517697074\n",
      "0.7048665495109122\n",
      "1.0\n",
      "0.7911434553988395\n",
      "0.6194274225257378\n",
      "1.0\n",
      "0.7601933920987806\n",
      "0.8634438349876061\n",
      "0.4406950585492932\n",
      "0.5718755155540661\n",
      "1.0\n",
      "0.7965100773735325\n",
      "0.5731273379375612\n",
      "0.9432379215722527\n",
      "0.4995751359652994\n",
      "0.7708688724052365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9134166588977729\n",
      "0.8532721558012799\n",
      "0.6873332711195502\n",
      "0.7245949812329288\n",
      "0.7081287626162236\n",
      "0.5694969487670277\n",
      "0.6431637888166242\n",
      "0.6381210717697404\n",
      "0.8254612838486164\n",
      "0.7606971932189103\n",
      "0.9095430364058891\n",
      "0.3333333333333333\n",
      "0.8169680738235195\n",
      "0.8645073774981995\n",
      "0.38224261473290666\n",
      "0.3333333333333333\n",
      "0.5263705276152086\n",
      "0.5250842331427514\n",
      "0.38559884598578353\n",
      "0.6607868248473735\n",
      "1.0\n",
      "0.7182833344196091\n",
      "1.0\n",
      "1.0\n",
      "0.48317417546358005\n",
      "0.9368077570869662\n",
      "0.45922519590939903\n",
      "0.5817271694072151\n",
      "0.540114229097692\n",
      "0.8340611153182703\n",
      "0.4434655591399216\n",
      "0.4066972554326934\n",
      "0.9095430364058891\n",
      "0.8437289933844668\n",
      "0.6651975125779157\n",
      "0.9134166588977729\n",
      "0.7013133146827744\n",
      "0.9402879873227596\n",
      "0.4342648940640067\n",
      "0.6975049503058763\n",
      "0.640742871878129\n",
      "0.5752317468069429\n",
      "0.5210914684875841\n",
      "0.3333333333333333\n",
      "0.9368077570869662\n",
      "0.3333333333333333\n",
      "0.955830517697074\n",
      "0.5926538996677988\n",
      "0.7625122742569328\n",
      "0.7606971932189103\n",
      "0.6350677584671\n",
      "0.654356660362087\n",
      "0.8010066431488297\n",
      "0.6813552397582848\n",
      "0.9477344873475498\n",
      "1.0\n",
      "0.7919672818521463\n",
      "1.0\n",
      "1.0\n",
      "0.8376318198599177\n",
      "0.8803485966094551\n",
      "0.7601933920987806\n",
      "0.9035650050446238\n",
      "1.0\n",
      "0.92663607790064\n",
      "0.9537125187088151\n",
      "0.5926538996677988\n",
      "0.7747424948971201\n",
      "1.0\n",
      "0.4184756332276246\n",
      "0.703935114791163\n",
      "0.8506629589898831\n",
      "0.7172640076381995\n",
      "0.5368232559598008\n",
      "0.7430580004525728\n",
      "0.9510907186004267\n",
      "0.9368077570869662\n",
      "0.3480974911723065\n",
      "0.70843168056646\n",
      "0.7476660540889449\n",
      "0.9134166588977729\n",
      "0.7625122742569328\n",
      "0.5184696683791956\n",
      "0.6826196654387717\n",
      "1.0\n",
      "0.9432379215722527\n",
      "0.7424350571691596\n",
      "0.9477344873475498\n",
      "0.6371857574553588\n",
      "1.0\n",
      "1.0\n",
      "0.7898916330153443\n",
      "0.92663607790064\n",
      "1.0\n",
      "0.6826196654387717\n",
      "0.9074250374176304\n",
      "0.5817271694072151\n",
      "0.9432379215722527\n",
      "0.4297683282887096\n",
      "1.0\n",
      "0.688597696800037\n",
      "0.7919672818521463\n",
      "1.0\n",
      "1.0\n",
      "0.4806182248640007\n",
      "0.7640796178876559\n",
      "0.5733705712589114\n",
      "1.0\n",
      "1.0\n",
      "0.43518692560571726\n",
      "0.6533354680029503\n",
      "0.9477344873475498\n",
      "0.7799082337019199\n",
      "0.8632555551147044\n",
      "0.5322996497667224\n",
      "0.9575861412006988\n",
      "0.7322633779828334\n",
      "0.9477344873475498\n",
      "0.4769219861846581\n",
      "0.6590312013437485\n",
      "0.6150499989137242\n",
      "0.5718755155540661\n",
      "0.5733705712589114\n",
      "0.9510907186004267\n",
      "1.0\n",
      "1.0\n",
      "0.8420631275202292\n",
      "0.5159958223394444\n",
      "0.7165277109159842\n",
      "0.5842837114006181\n",
      "0.8803485966094551\n",
      "1.0\n",
      "1.0\n",
      "0.9575861412006988\n",
      "0.8446849276286176\n",
      "0.5645688697974787\n",
      "0.48006117753205346\n",
      "0.7117879118193369\n",
      "0.6321430404084751\n",
      "0.6873332711195502\n",
      "1.0\n",
      "0.7284550136059355\n",
      "0.7817956026658202\n",
      "0.4280127047850847\n",
      "0.6873332711195502\n",
      "0.6873332711195502\n",
      "0.6676184295164108\n",
      "0.7276427210494697\n",
      "1.0\n",
      "0.42853009602409153\n",
      "0.6912068936114338\n",
      "0.6681358207554177\n",
      "1.0\n",
      "0.7032659417616602\n",
      "0.5289578556346352\n",
      "0.7670088400322299\n",
      "1.0\n",
      "0.6321430404084751\n",
      "0.7619490156024055\n",
      "1.0\n",
      "0.49172233893712536\n",
      "1.0\n",
      "0.8634438349876061\n",
      "1.0\n",
      "0.4237902969274442\n",
      "0.6613374802049091\n",
      "0.8909724089198024\n",
      "0.3333333333333333\n",
      "0.43638289305226546\n",
      "0.7601933920987806\n",
      "0.5159958223394444\n",
      "0.5859240426513556\n",
      "0.9069212362975007\n",
      "1.0\n",
      "0.9537125187088151\n",
      "1.0\n",
      "0.9074250374176304\n",
      "0.6799978653303831\n",
      "1.0\n",
      "0.7284550136059355\n",
      "0.8010066431488297\n",
      "0.7830474250493153\n",
      "0.8043628744017065\n",
      "0.6639592803132975\n",
      "1.0\n",
      "0.9477344873475498\n",
      "0.5328178880076417\n",
      "0.6742368738745992\n",
      "0.7965100773735325\n",
      "0.5821032065260975\n",
      "0.7580753931105217\n",
      "0.8988252059479764\n",
      "0.6909039756611977\n",
      "0.9477344873475498\n",
      "0.8254612838486164\n",
      "0.7585927843495286\n",
      "0.5210914684875841\n",
      "0.92663607790064\n",
      "0.8698739994728926\n",
      "0.6889177221332141\n",
      "0.6507136678945618\n",
      "0.7606971932189103\n",
      "0.8926382747840401\n",
      "0.604426186603377\n",
      "0.5289442655157581\n",
      "0.7606971932189103\n",
      "0.8800456786592188\n",
      "0.9432379215722527\n",
      "0.42590829591570295\n",
      "0.5031322503880696\n",
      "1.0\n",
      "1.0\n",
      "0.9368077570869662\n",
      "0.3333333333333333\n",
      "0.7745542150242184\n",
      "0.618923621405608\n",
      "1.0\n",
      "0.7494881649378594\n",
      "0.740982351615771\n",
      "0.5603464619398382\n",
      "0.48079560867654175\n",
      "1.0\n",
      "0.8803485966094551\n",
      "0.46620415572674534\n",
      "0.499272218015063\n",
      "0.8043628744017065\n",
      "0.92663607790064\n",
      "0.8824665955977139\n",
      "0.6629354803709439\n",
      "0.6907156957882958\n",
      "0.8208416963154033\n",
      "0.45375356711045917\n",
      "0.8501591578697533\n",
      "0.7965100773735325\n",
      "1.0\n",
      "0.7451759994408317\n",
      "0.8176084868204424\n",
      "0.9575861412006988\n",
      "0.47430018607626956\n",
      "0.43250927056038185\n",
      "0.8340611153182703\n",
      "0.9510907186004267\n",
      "0.4721957772068878\n",
      "0.6129319999254654\n",
      "0.3480974911723065\n",
      "0.8083682423538656\n",
      "0.8743705652481898\n",
      "0.8532721558012799\n",
      "0.8585157560180571\n",
      "0.9368077570869662\n",
      "0.8356329630349425\n",
      "1.0\n",
      "0.8527809579781418\n",
      "1.0\n",
      "0.6241410282065163\n",
      "0.811178322335156\n",
      "0.7373232024126578\n",
      "0.6447356365332964\n",
      "0.9537125187088151\n",
      "0.8585157560180571\n",
      "1.0\n",
      "1.0\n",
      "0.4690157659730023\n",
      "0.7874707160768492\n",
      "0.8634438349876061\n",
      "0.6431637888166242\n",
      "0.8043628744017065\n",
      "0.6410593799472425\n",
      "0.9035650050446238\n",
      "0.8632555551147044\n",
      "0.7445530561574184\n",
      "1.0\n",
      "0.4831089173485944\n",
      "0.7224117241296842\n",
      "0.8091026734983539\n",
      "1.0\n",
      "0.8192743526846801\n",
      "0.5731273379375612\n",
      "0.584369627369062\n",
      "1.0\n",
      "1.0\n",
      "0.8463507934928552\n",
      "0.5726361401144231\n",
      "0.9575861412006988\n",
      "0.9432379215722527\n",
      "1.0\n",
      "0.7770874137562999\n",
      "0.8800456786592188\n",
      "0.5902753328807605\n",
      "0.7900799128882461\n",
      "0.8606337550063158\n",
      "0.8800456786592188\n",
      "0.4911705278921376\n",
      "0.604426186603377\n",
      "0.6381210717697404\n",
      "0.7055069625078352\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-67e0ffa7535e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_rel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdoc_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mn_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mn_irr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_docs\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mn_rel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def idcg(n_rel):\n",
    "    # Assuming binary relevance.\n",
    "    nums = np.ones(n_rel)\n",
    "    denoms = np.log2(np.arange(n_rel) + 1 + 1)\n",
    "    return (nums / denoms).sum()\n",
    "\n",
    "for epoch in range(3):\n",
    "    for labels,docs,n_rel in dataset:\n",
    "        doc_scores = model(docs)\n",
    "        n_docs = len(labels)\n",
    "        n_irr = n_docs- n_rel\n",
    "        if n_rel==0:\n",
    "            continue\n",
    "        #forward_compute\n",
    "        doc_scores = model(docs)\n",
    "       \n",
    "\n",
    "        # Document ranks.\n",
    "        (sorted_scores, sorted_idxs) = doc_scores.sort(dim = 0, descending = True)\n",
    "        doc_ranks = torch.zeros(n_docs,dtype=torch.float).to(device)\n",
    "        doc_ranks[sorted_idxs] = 1.0 + torch.arange(n_docs).view((n_docs, 1)).float().to(device)\n",
    "        doc_ranks = doc_ranks.view((n_docs, 1))\n",
    "\n",
    "        # # Compute lambdas.\n",
    "        diffs = doc_scores[:n_rel] - doc_scores[n_rel:].view(n_irr)\n",
    "        exped = diffs.exp()\n",
    "        # # See equation (6) in [2].\n",
    "        N = 1 / idcg(n_rel)\n",
    "        #print('idgc =',N)\n",
    "        ndcg_diffs = (1 / (1 + doc_ranks[:n_rel])).log2() - (1 / (1 + doc_ranks[n_rel:])).log2().view(n_irr)\n",
    "        lamb_updates = -1 / (1 + exped) * N * ndcg_diffs.abs()\n",
    "        # # See section 6.1 in [1], but lambdas have opposite signs from [2].\n",
    "        lambs = torch.zeros((n_docs, 1)).to(device)\n",
    "        lambs[:n_rel] -= lamb_updates.sum(dim = 1, keepdim = True)\n",
    "        lambs[n_rel:] += lamb_updates.sum(dim = 0, keepdim = True).t()\n",
    "\n",
    "        if epoch%100==0:\n",
    "            print(ndcg_score([1]*n_rel+[2]*n_irr,doc_ranks.squeeze(1).tolist()))\n",
    "        # # Accumulate lambda scaled gradients.\n",
    "        model.zero_grad()\n",
    "        doc_scores.backward(lambs)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vqa]",
   "language": "python",
   "name": "conda-env-vqa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
