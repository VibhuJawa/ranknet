{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Michael A. Alcorn (malcorn@redhat.com)\n",
    "# A (slightly modified) implementation of LamdaRank as described in [1].\n",
    "#   [1] https://papers.nips.cc/paper/2971-learning-to-rank-with-nonsmooth-cost-functions.pdf\n",
    "#   [2] https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Mathieu Blondel, November 2013\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def dcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    \"\"\"Discounted cumulative gain (DCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score : array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "    gains : str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    DCG @k : float\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    if gains == \"exponential\":\n",
    "        gains = 2 ** y_true - 1\n",
    "    elif gains == \"linear\":\n",
    "        gains = y_true\n",
    "    else:\n",
    "        raise ValueError(\"Invalid gains option.\")\n",
    "\n",
    "    # highest rank is 1 so +2 instead of +1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    \"\"\"Normalized discounted cumulative gain (NDCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score : array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "    gains : str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    NDCG @k : float\n",
    "    \"\"\"\n",
    "    best = dcg_score(y_true, y_true, k, gains)\n",
    "    actual = dcg_score(y_true, y_score, k, gains)\n",
    "    return actual / best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ndcg_score([1,2,3],[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "input_dim = 46\n",
    "data_file = 'data/MQ2007/Fold1/train.txt'\n",
    "data_dir = 'data/MQ2007/Fold1/q_json'\n",
    "data_meta_csv = \"{}/metafile.csv\".format(data_dir)\n",
    "\n",
    "feats_to_drop = ['doc_id','inc','prob','qid','y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# couple of days\n",
    "# day that you recieve the offer \n",
    "# email: \n",
    "# 2 weeks\n",
    "# location: santa-calara, austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_tensor = torch.tensor([1,0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(label_tensor==0).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RANKNET_DS(Dataset):\n",
    "    \"\"\"Document Ranking Dataset.\"\"\"\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file (string): Path to the txt file with q_id.\n",
    "            root_dir (string): Directory with all the query_details.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.meta_file = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.feats_to_drop = feats_to_drop\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_fname = os.path.join(self.root_dir,str(self.meta_file.iloc[idx]['qid']))\n",
    "        q_data = pd.read_csv(\"{}.csv\".format(q_fname))\n",
    "        if self.transform:\n",
    "            return self.transform(q_data,self.feats_to_drop)\n",
    "            \n",
    "        return q_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7cc938dc77ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mranknet_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRANKNET_DS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_meta_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranknet_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transform' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "ranknet_ds = RANKNET_DS(data_meta_csv,data_dir,transform)\n",
    "label,data = ranknet_ds[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(q_sample,cols_to_drop):\n",
    "    \"\"\"\n",
    "        input dataframe\n",
    "        transforms datafram into tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    label_tensor = torch.tensor(q_sample['y'].values)\n",
    "    n_rel = (label_tensor!=0).sum().item()\n",
    "    data_tensor = torch.tensor(q_sample[q_sample.columns.difference(cols_to_drop)].values).float()\n",
    "    return label_tensor,data_tensor,n_rel\n",
    "    \n",
    "class DOC_RANK(Dataset):\n",
    "    \"\"\"Document Ranking Dataset.\"\"\"\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file (string): Path to the txt file with q_id.\n",
    "            root_dir (string): Directory with all the query_details.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.meta_file = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.feats_to_drop = feats_to_drop\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_fname = os.path.join(self.root_dir,str(self.meta_file.iloc[idx]['qid']))\n",
    "        q_data = pd.read_csv(\"{}.csv\".format(q_fname))\n",
    "        if self.transform:\n",
    "            return self.transform(q_data,self.feats_to_drop)\n",
    "            \n",
    "        return q_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label,data = dataset[13]\n",
    "# print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(input_dim, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DOC_RANK(data_meta_csv,data_dir,transform)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (0) must match the size of tensor b (40) at non-singleton dimension 0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-4025d9317a20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# # Compute lambdas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_rel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdoc_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_rel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_irr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mexped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiffs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# # See equation (6) in [2].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (0) must match the size of tensor b (40) at non-singleton dimension 0"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def idcg(n_rel):\n",
    "    # Assuming binary relevance.\n",
    "    nums = np.ones(n_rel)\n",
    "    denoms = np.log2(np.arange(n_rel) + 1 + 1)\n",
    "    return (nums / denoms).sum()\n",
    "\n",
    "for epoch in range(3):\n",
    "    for labels,docs,n_rel in dataset:\n",
    "        doc_scores = model(docs)\n",
    "        n_docs = len(labels)\n",
    "        n_irr = n_docs- n_rel\n",
    "        #forward_compute\n",
    "        doc_scores = model(docs)\n",
    "\n",
    "        # Document ranks.\n",
    "        (sorted_scores, sorted_idxs) = doc_scores.sort(dim = 0, descending = True)\n",
    "        doc_ranks = torch.zeros(n_docs,dtype=torch.float).to(device)\n",
    "        doc_ranks[sorted_idxs] = 1.0 + torch.arange(n_docs).view((n_docs, 1)).float().to(device)\n",
    "        doc_ranks = doc_ranks.view((n_docs, 1))\n",
    "\n",
    "        # # Compute lambdas.\n",
    "        diffs = doc_scores[:n_rel] - doc_scores[n_rel:].view(n_irr)\n",
    "        exped = diffs.exp()\n",
    "        # # See equation (6) in [2].\n",
    "        N = 1 / idcg(n_rel)\n",
    "        #print('idgc =',N)\n",
    "        ndcg_diffs = (1 / (1 + doc_ranks[:n_rel])).log2() - (1 / (1 + doc_ranks[n_rel:])).log2().view(n_irr)\n",
    "        lamb_updates = -1 / (1 + exped) * N * ndcg_diffs.abs()\n",
    "        # # See section 6.1 in [1], but lambdas have opposite signs from [2].\n",
    "        lambs = torch.zeros((n_docs, 1)).to(device)\n",
    "        lambs[:n_rel] -= lamb_updates.sum(dim = 1, keepdim = True)\n",
    "        lambs[n_rel:] += lamb_updates.sum(dim = 0, keepdim = True).t()\n",
    "\n",
    "        if i%100==0:\n",
    "            print(ndcg_score([1]*n_rel+[2]*n_irr,doc_ranks.squeeze(1).tolist()))\n",
    "        # # Accumulate lambda scaled gradients.\n",
    "        model.zero_grad()\n",
    "        doc_scores.backward(lambs)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_irr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([38,  8, 29, 10, 13, 22, 15,  1, 21, 26, 32, 24, 39,  4, 14, 19, 34, 28,\n",
       "        12, 33,  7, 37, 30, 16,  9,  3, 11,  2,  5, 36, 18, 35,  6, 23, 31, 25,\n",
       "        20,  0, 27, 17])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_idxs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [11],\n",
       "        [12],\n",
       "        [13],\n",
       "        [14],\n",
       "        [15],\n",
       "        [16],\n",
       "        [17],\n",
       "        [18],\n",
       "        [19],\n",
       "        [20]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+torch.arange(n_docs).view((n_docs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vqa]",
   "language": "python",
   "name": "conda-env-vqa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}