{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   [1] https://papers.nips.cc/paper/2971-learning-to-rank-with-nonsmooth-cost-functions.pdf\n",
    "#   [2] https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (C) Mathieu Blondel, November 2013\n",
    "# License: BSD 3 clause\n",
    "# metrics\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def dcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    \"\"\"Discounted cumulative gain (DCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score : array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "    gains : str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    DCG @k : float\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    if gains == \"exponential\":\n",
    "        gains = 2 ** y_true - 1\n",
    "    elif gains == \"linear\":\n",
    "        gains = y_true\n",
    "    else:\n",
    "        raise ValueError(\"Invalid gains option.\")\n",
    "\n",
    "    # highest rank is 1 so +2 instead of +1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    \"\"\"Normalized discounted cumulative gain (NDCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score : array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "    gains : str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    NDCG @k : float\n",
    "    \"\"\"\n",
    "    best = dcg_score(y_true, y_true, k, gains)\n",
    "    actual = dcg_score(y_true, y_score, k, gains)\n",
    "    return actual / best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    \"\"\"Discounted cumulative gain (DCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score : array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "    gains : str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    DCG @k : float\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    if gains == \"exponential\":\n",
    "        gains = 2 ** y_true - 1\n",
    "    elif gains == \"linear\":\n",
    "        gains = y_true\n",
    "    else:\n",
    "        raise ValueError(\"Invalid gains option.\")\n",
    "\n",
    "    # highest rank is 1 so +2 instead of +1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    \"\"\"Normalized discounted cumulative gain (NDCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score : array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "    gains : str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    NDCG @k : float\n",
    "    \"\"\"\n",
    "    best = dcg_score(y_true, y_true, k, gains)\n",
    "    actual = dcg_score(y_true, y_score, k, gains)\n",
    "    return actual / best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "input_dim = 46\n",
    "data_file = 'data/MQ2007/Fold1/train.txt'\n",
    "data_train_dir = 'data/MQ2007/Fold1/train_json'\n",
    "data_train_meta_csv = \"{}/metafile.csv\".format(data_train_dir)\n",
    "\n",
    "data_test_dir = 'data/MQ2007/Fold1/test_json'\n",
    "data_test_meta_csv = \"{}/metafile.csv\".format(data_test_dir)\n",
    "\n",
    "\n",
    "feats_to_drop = ['doc_id','inc','prob','qid','y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [str(i) for i in range(1,47)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(q_sample,cols_to_drop):\n",
    "    \"\"\"\n",
    "        input dataframe\n",
    "        transforms datafram into tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    label_tensor = torch.tensor(int(q_sample['y']))\n",
    "    data_tensor = torch.tensor(q_sample[feature_cols].values.astype('float')).float()\n",
    "    return {'y':label_tensor,'data':data_tensor}\n",
    "\n",
    "class RANKNET_TRAIN_DS(Dataset):\n",
    "    \"\"\"Document Ranking Dataset.\"\"\"\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file (string): Path to the txt file with q_id.\n",
    "            root_dir (string): Directory with all the query_details.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.meta_file = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.feats_to_drop = feats_to_drop\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_fname = os.path.join(self.root_dir,str(self.meta_file.iloc[idx]['qid']))\n",
    "        q_data = pd.read_csv(\"{}.csv\".format(q_fname))\n",
    "        i1,i2 = np.random.choice(len(q_data),2)\n",
    "        z1,z2 = q_data.iloc[i1],q_data.iloc[i2]\n",
    "        sample = {'doc1':transform(z1,feats_to_drop),'doc2':transform(z2,feats_to_drop)}\n",
    "        return sample \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ls(q_sample_ls,cols_to_drop):\n",
    "    \"\"\"\n",
    "        input dataframe\n",
    "        transforms datafram into tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    label_tensor_ls = torch.tensor(np.asarray([q_sample['y'] for q_sample in q_sample_ls]))\n",
    "    data_tensor_ls = torch.tensor(\\\n",
    "                                  np.asarray([q_sample[feature_cols].values.astype('float') \\\n",
    "                                              for q_sample in q_sample_ls ])).float()\n",
    "    return {'y':label_tensor_ls,'data':data_tensor_ls}\n",
    "\n",
    "class RANKNET_TEST_DS(Dataset):\n",
    "    \"\"\"Document Ranking Dataset.\"\"\"\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the txt file with q_id.\n",
    "            root_dir (string): Directory with all the query_details.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.meta_file = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.feats_to_drop = feats_to_drop\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_fname = os.path.join(self.root_dir,str(self.meta_file.iloc[idx]['qid']))\n",
    "        q_data = pd.read_csv(\"{}.csv\".format(q_fname))\n",
    "        z_ls = [q_data.iloc[i] for i in range(len(q_data))]\n",
    "        sample_ls = transform_ls(z_ls,self.feats_to_drop)\n",
    "        return sample_ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "ranknet_train_ds = RANKNET_TRAIN_DS(data_train_meta_csv,data_train_dir,transform)\n",
    "ranknet_test_ds = RANKNET_TEST_DS(data_test_meta_csv,data_test_dir,transform_ls)\n",
    "train_dataloader = DataLoader(ranknet_train_ds, batch_size=batch_size,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranknet_test_ds[0]['data']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.\n",
    "class RankNet(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "            super(RankNet, self).__init__()\n",
    "            self.l1 = nn.Linear(input_dim, 128)\n",
    "            self.l2 = nn.Linear(128, 64)\n",
    "            self.l3 = nn.Linear(64, 32)\n",
    "            self.l4 = nn.Linear(32, 1)\n",
    "    \n",
    "    def single_forward(self,x):\n",
    "        return self.l4(F.relu(self.l3(F.relu(self.l2(F.relu(self.l1(x)))))))\n",
    "    \n",
    "    def forward(self, x_i, x_j, t_i, t_j):\n",
    "        \n",
    "       \n",
    "        s_i = self.single_forward(x_i)\n",
    "        s_j = self.single_forward(x_j)\n",
    "        s_diff = s_i - s_j\n",
    "        s_diff = s_diff.squeeze(1)\n",
    "        S_ij = torch.zeros(size = t_i.shape)\n",
    "        pos_mask = t_i>t_j\n",
    "        neg_mask = t_i<t_j\n",
    "        equal_mask = t_i==t_j\n",
    "        S_ij[pos_mask]=1 \n",
    "        S_ij[neg_mask]=-1\n",
    "        S_ij[equal_mask]=0\n",
    "\n",
    "        term1 = (1 - S_ij) * s_diff\n",
    "        \n",
    "        loss = (1 - S_ij) * s_diff /2.0 + torch.log(1 + torch.exp(-s_diff))\n",
    "        return loss\n",
    "    \n",
    "    def predict(self,x):\n",
    "        return self.single_forward(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RankNet(input_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.693467378616333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/ipykernel/__main__.py:50: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Test DCG: 0.2804546738575022\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 1 Train Loss: 0.6933716833591461\n",
      "Epoch: 2 Train Loss: 0.692994948476553\n",
      "Epoch: 3 Train Loss: 0.6925122886896133\n",
      "Epoch: 4 Train Loss: 0.6924650147557259\n",
      "Epoch: 5 Train Loss: 0.6925134211778641\n",
      "Epoch: 6 Train Loss: 0.6916909851133823\n",
      "Epoch: 7 Train Loss: 0.6915219016373158\n",
      "Epoch: 8 Train Loss: 0.6913166753947735\n",
      "Epoch: 9 Train Loss: 0.6916309185326099\n",
      "Epoch: 10 Train Loss: 0.6907091774046421\n",
      "Epoch: 10 Test DCG: 0.5197877513504553\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 11 Train Loss: 0.6906305216252804\n",
      "Epoch: 12 Train Loss: 0.6896536350250244\n",
      "Epoch: 13 Train Loss: 0.6896859966218472\n",
      "Epoch: 14 Train Loss: 0.6880596876144409\n",
      "Epoch: 15 Train Loss: 0.686484944075346\n",
      "Epoch: 16 Train Loss: 0.6878590956330299\n",
      "Epoch: 17 Train Loss: 0.6851712130010128\n",
      "Epoch: 18 Train Loss: 0.6870881877839565\n",
      "Epoch: 19 Train Loss: 0.6868491768836975\n",
      "Epoch: 20 Train Loss: 0.6871009506285191\n",
      "Epoch: 20 Test DCG: 0.5232495608509108\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 21 Train Loss: 0.6840903982520103\n",
      "Epoch: 22 Train Loss: 0.6823489144444466\n",
      "Epoch: 23 Train Loss: 0.6846051178872585\n",
      "Epoch: 24 Train Loss: 0.6850259006023407\n",
      "Epoch: 25 Train Loss: 0.6864417642354965\n",
      "Epoch: 26 Train Loss: 0.6837141141295433\n",
      "Epoch: 27 Train Loss: 0.6845598295331001\n",
      "Epoch: 28 Train Loss: 0.6846493259072304\n",
      "Epoch: 29 Train Loss: 0.684472069144249\n",
      "Epoch: 30 Train Loss: 0.6824448779225349\n",
      "Epoch: 30 Test DCG: 0.5315516833066658\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 31 Train Loss: 0.6833092048764229\n",
      "Epoch: 32 Train Loss: 0.683922816067934\n",
      "Epoch: 33 Train Loss: 0.6844594329595566\n",
      "Epoch: 34 Train Loss: 0.6828191056847572\n",
      "Epoch: 35 Train Loss: 0.6866172440350056\n",
      "Epoch: 36 Train Loss: 0.684498555958271\n",
      "Epoch: 37 Train Loss: 0.6869656518101692\n",
      "Epoch: 38 Train Loss: 0.6854251772165298\n",
      "Epoch: 39 Train Loss: 0.681649386882782\n",
      "Epoch: 40 Train Loss: 0.6836491078138351\n",
      "Epoch: 40 Test DCG: 0.5417396840427332\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 41 Train Loss: 0.6828813478350639\n",
      "Epoch: 42 Train Loss: 0.6836910024285316\n",
      "Epoch: 43 Train Loss: 0.6859077662229538\n",
      "Epoch: 44 Train Loss: 0.6865443661808968\n",
      "Epoch: 45 Train Loss: 0.6795639134943485\n",
      "Epoch: 46 Train Loss: 0.680949542671442\n",
      "Epoch: 47 Train Loss: 0.6841591596603394\n",
      "Epoch: 48 Train Loss: 0.6825562380254269\n",
      "Epoch: 49 Train Loss: 0.6830693483352661\n",
      "Epoch: 50 Train Loss: 0.6819304190576077\n",
      "Epoch: 50 Test DCG: 0.5443643471884438\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 51 Train Loss: 0.6805938184261322\n",
      "Epoch: 52 Train Loss: 0.6799026392400265\n",
      "Epoch: 53 Train Loss: 0.6859947890043259\n",
      "Epoch: 54 Train Loss: 0.6834953054785728\n",
      "Epoch: 55 Train Loss: 0.6833202913403511\n",
      "Epoch: 56 Train Loss: 0.6807849407196045\n",
      "Epoch: 57 Train Loss: 0.6840171962976456\n",
      "Epoch: 58 Train Loss: 0.6803367286920547\n",
      "Epoch: 59 Train Loss: 0.6827418804168701\n",
      "Epoch: 60 Train Loss: 0.6806365177035332\n",
      "Epoch: 60 Test DCG: 0.545434015994934\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 61 Train Loss: 0.6813714317977428\n",
      "Epoch: 62 Train Loss: 0.680556170642376\n",
      "Epoch: 63 Train Loss: 0.6799260079860687\n",
      "Epoch: 64 Train Loss: 0.6797577738761902\n",
      "Epoch: 65 Train Loss: 0.6817954368889332\n",
      "Epoch: 66 Train Loss: 0.680672038346529\n",
      "Epoch: 67 Train Loss: 0.6849143616855145\n",
      "Epoch: 68 Train Loss: 0.6811300218105316\n",
      "Epoch: 69 Train Loss: 0.6818966567516327\n",
      "Epoch: 70 Train Loss: 0.686923336237669\n",
      "Epoch: 70 Test DCG: 0.5545493873973719\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 71 Train Loss: 0.6812997199594975\n",
      "Epoch: 72 Train Loss: 0.6862424947321415\n",
      "Epoch: 73 Train Loss: 0.6821442544460297\n",
      "Epoch: 74 Train Loss: 0.680430319160223\n",
      "Epoch: 75 Train Loss: 0.6840029582381248\n",
      "Epoch: 76 Train Loss: 0.6828490197658539\n",
      "Epoch: 77 Train Loss: 0.680706687271595\n",
      "Epoch: 78 Train Loss: 0.6815096363425255\n",
      "Epoch: 79 Train Loss: 0.6812990382313728\n",
      "Epoch: 80 Train Loss: 0.6853491552174091\n",
      "Epoch: 80 Test DCG: 0.5506408636823574\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 81 Train Loss: 0.68210219591856\n",
      "Epoch: 82 Train Loss: 0.6844797059893608\n",
      "Epoch: 83 Train Loss: 0.6826825179159641\n",
      "Epoch: 84 Train Loss: 0.6812159568071365\n",
      "Epoch: 85 Train Loss: 0.6820613257586956\n",
      "Epoch: 86 Train Loss: 0.6831493824720383\n",
      "Epoch: 87 Train Loss: 0.6832348667085171\n",
      "Epoch: 88 Train Loss: 0.6809400394558907\n",
      "Epoch: 89 Train Loss: 0.6837043389678001\n",
      "Epoch: 90 Train Loss: 0.6823057942092419\n",
      "Epoch: 90 Test DCG: 0.548029601441817\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 91 Train Loss: 0.6853564232587814\n",
      "Epoch: 92 Train Loss: 0.6826064363121986\n",
      "Epoch: 93 Train Loss: 0.6854330524802208\n",
      "Epoch: 94 Train Loss: 0.6858998574316502\n",
      "Epoch: 95 Train Loss: 0.6814500689506531\n",
      "Epoch: 96 Train Loss: 0.6813731119036674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-389:\n",
      "Process Process-391:\n",
      "Process Process-392:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-390:\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-7-e354d8ce6931>\", line 31, in __getitem__\n",
      "    q_data = pd.read_csv(\"{}.csv\".format(q_fname))\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/io/parsers.py\", line 678, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"<ipython-input-7-e354d8ce6931>\", line 34, in __getitem__\n",
      "    sample = {'doc1':transform(z1,feats_to_drop),'doc2':transform(z2,feats_to_drop)}\n",
      "  File \"<ipython-input-7-e354d8ce6931>\", line 31, in __getitem__\n",
      "    q_data = pd.read_csv(\"{}.csv\".format(q_fname))\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/io/parsers.py\", line 446, in _read\n",
      "    data = parser.read(nrows)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/io/parsers.py\", line 678, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/io/parsers.py\", line 446, in _read\n",
      "    data = parser.read(nrows)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1036, in read\n",
      "    ret = self._engine.read(nrows)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1848, in read\n",
      "    data = self._reader.read(nrows)\n",
      "  File \"<ipython-input-7-e354d8ce6931>\", line 8, in transform\n",
      "    data_tensor = torch.tensor(q_sample[feature_cols].values.astype('float')).float()\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1036, in read\n",
      "    ret = self._engine.read(nrows)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1848, in read\n",
      "    data = self._reader.read(nrows)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 876, in pandas._libs.parsers.TextReader.read\n",
      "  File \"pandas/_libs/parsers.pyx\", line 891, in pandas._libs.parsers.TextReader._read_low_memory\n",
      "  File \"pandas/_libs/parsers.pyx\", line 876, in pandas._libs.parsers.TextReader.read\n",
      "  File \"pandas/_libs/parsers.pyx\", line 891, in pandas._libs.parsers.TextReader._read_low_memory\n",
      "  File \"pandas/_libs/parsers.pyx\", line 968, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"pandas/_libs/parsers.pyx\", line 1094, in pandas._libs.parsers.TextReader._convert_column_data\n",
      "  File \"pandas/_libs/parsers.pyx\", line 968, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/series.py\", line 810, in __getitem__\n",
      "    return self._get_with(key)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 1134, in pandas._libs.parsers.TextReader._convert_tokens\n",
      "  File \"pandas/_libs/parsers.pyx\", line 1094, in pandas._libs.parsers.TextReader._convert_column_data\n",
      "  File \"pandas/_libs/parsers.pyx\", line 1180, in pandas._libs.parsers.TextReader._convert_with_dtype\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/series.py\", line 851, in _get_with\n",
      "    return self.loc[key]\n",
      "  File \"pandas/_libs/parsers.pyx\", line 1134, in pandas._libs.parsers.TextReader._convert_tokens\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/dtypes/common.py\", line 811, in is_integer_dtype\n",
      "    def is_integer_dtype(arr_or_dtype):\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1478, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 1180, in pandas._libs.parsers.TextReader._convert_with_dtype\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/dtypes/common.py\", line 855, in is_integer_dtype\n",
      "    not issubclass(tipo, (np.datetime64, np.timedelta64)))\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1901, in _getitem_axis\n",
      "    return self._getitem_iterable(key, axis=axis)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1145, in _getitem_iterable\n",
      "    d = {axis: [ax.reindex(keyarr)[0], indexer]}\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3622, in reindex\n",
      "    tolerance=tolerance)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3232, in get_indexer\n",
      "    pself, ptarget = self._maybe_promote(target)\n",
      "  File \"<ipython-input-7-e354d8ce6931>\", line 33, in __getitem__\n",
      "    z1,z2 = q_data.iloc[i1],q_data.iloc[i2]\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3391, in _maybe_promote\n",
      "    from pandas.core.indexes.datetimes import DatetimeIndex\n",
      "  File \"<frozen importlib._bootstrap>\", line 1019, in _handle_fromlist\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1478, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2104, in _getitem_axis\n",
      "    return self._get_loc(key, axis=axis)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/indexing.py\", line 145, in _get_loc\n",
      "    return self.obj._ixs(key, axis=axis)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/frame.py\", line 2616, in _ixs\n",
      "    new_values = self._data.fast_xs(i)\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/internals.py\", line 4074, in fast_xs\n",
      "    dtype = _interleaved_dtype(self.blocks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/internals.py\", line 5048, in _interleaved_dtype\n",
      "    dtype = find_common_type([b.dtype for b in blocks])\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\", line 1141, in find_common_type\n",
      "    return np.find_common_type(types, [])\n",
      "  File \"/Users/jawa/anaconda3/envs/vqa/lib/python3.7/site-packages/numpy/core/numerictypes.py\", line 1002, in find_common_type\n",
      "    scalar_types = [dtype(x) for x in scalar_types]\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e949c8d62bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {} Train Loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e949c8d62bb0>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mepoch_loss_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_step(model):\n",
    "    epoch_loss_ls = []\n",
    "    for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "        xi, ti = sample_batched['doc1']['data'], sample_batched['doc1']['y']\n",
    "        xj, tj =  sample_batched['doc2']['data'], sample_batched['doc2']['y']\n",
    "        loss = model(xi,xj,ti,tj)\n",
    "        total_loss = loss.mean()\n",
    "        epoch_loss_ls.append(total_loss.item())\n",
    "        model.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return sum(epoch_loss_ls)/len(epoch_loss_ls)\n",
    "        \n",
    "def test_step(model):\n",
    "    ndgc_ls = []\n",
    "    for i_batch, sample_batched in enumerate(ranknet_test_ds):\n",
    "        label,data = sample_batched['y'],sample_batched['data']\n",
    "        pred = model.predict(data)\n",
    "        pred_ar = pred.squeeze(1).detach().numpy()\n",
    "        label_ar = label.detach().numpy()\n",
    "        ndgc_s = ndcg_score(label_ar,pred_ar)\n",
    "        if not math.isnan(ndgc_s):\n",
    "            ndgc_ls.append(ndgc_s)\n",
    "    \n",
    "    return sum(ndgc_ls)/len(ndgc_ls)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    epoch_train_loss = train_step(model)\n",
    "    print(\"Epoch: {} Train Loss: {}\".format(epoch,epoch_train_loss))\n",
    "    if epoch%10==0:\n",
    "        epoch_test_dcg = test_step(model)\n",
    "        print(\"Epoch: {} Test DCG: {}\".format(epoch,epoch_test_dcg))\n",
    "        print(\"--\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vqa]",
   "language": "python",
   "name": "conda-env-vqa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
